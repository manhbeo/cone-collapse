import numpy as np
import torch
import time
import os
from utils import plotting

class ConeCollapse:
    def __init__(self, theta=0.1, max_iters=100, tol=1e-6, batch_size=32,
                 verbose=True, plot=False, adaptive_theta=True, early_stopping=True,
                 device=None):
        """
        Initialize the PyTorch Cone Collapsing algorithm.

        Args:
            theta (float): Step size parameter for tilting vertex rays.
            max_iters (int): Maximum number of iterations.
            tol (float): Tolerance for convergence.
            batch_size (int): Batch size for processing data points.
            verbose (bool): Whether to print progress information.
            plot (bool): Whether to generate plots (only works for m=3).
            adaptive_theta (bool): Whether to use adaptive theta values.
            early_stopping (bool): Whether to use early stopping criteria.
            device (str): PyTorch device to use ('cuda', 'cpu', or None for auto-detection).
        """
        self.theta = theta
        self.max_iters = max_iters
        self.tol = tol
        self.batch_size = batch_size
        self.verbose = verbose
        self.plot = plot
        self.adaptive_theta = adaptive_theta
        self.early_stopping = early_stopping

        # Set device (use GPU if available)
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)

        if self.verbose:
            print(f"Using device: {self.device}")

    def projection(self, u, mu):
        """Calculate the projection of mu onto u using PyTorch."""
        dot_product = torch.dot(u, mu)
        return (dot_product / torch.dot(u, u)) * u

    def update_vector(self, ui, mu, theta):
        """
        Update the vector ui towards the vector mu by an angle theta.
        PyTorch implementation.
        """
        # Pre-compute dot products
        ui_dot_mu = torch.dot(ui, mu)
        ui_squared = torch.dot(ui, ui)

        # Project mu onto the span of ui
        mu_tilde = (ui_dot_mu / ui_squared) * ui

        # Calculate component of mu orthogonal to ui
        mu_perp = mu - mu_tilde

        # Calculate norms
        mu_perp_norm = torch.norm(mu_perp)
        mu_tilde_norm = torch.norm(mu_tilde)

        if mu_tilde_norm <= 1e-12 or mu_perp_norm <= 1e-12:
            # Avoid division by zero in case of degenerate situations
            return ui
        else:
            # Implement the formula from the paper
            tan_theta = torch.tan(torch.tensor(theta, device=self.device))
            ui_prime = mu_tilde + (mu_tilde_norm * tan_theta) * (mu_perp / mu_perp_norm)
            return ui_prime / torch.norm(ui_prime)

    def batch_update_vectors(self, U, mu, theta):
        """
        Update all vectors in U towards mu by angle theta in a batch operation.
        PyTorch implementation with vectorized operations.
        """
        # Get dimensions
        m, k = U.size()

        # Calculate all projections at once using batched operations
        dots = torch.matmul(U.t(), mu)  # Dot product of each column with mu
        norms_squared = torch.sum(U * U, dim=0)  # Squared norm of each column

        # Compute all mu_tilde vectors
        scale_factors = dots / norms_squared
        mu_tilde = U * scale_factors.unsqueeze(0)

        # Compute all mu_perp vectors
        mu_perp = mu.unsqueeze(1).expand(m, k) - mu_tilde

        # Calculate norms
        mu_tilde_norms = torch.norm(mu_tilde, dim=0)
        mu_perp_norms = torch.norm(mu_perp, dim=0)

        # Prepare for the update
        tan_theta = torch.tan(torch.tensor(theta, device=self.device))

        # Initialize the updated vectors with the original vectors
        U_new = U.clone()

        # Identify valid vectors to update (avoid division by zero)
        valid_indices = (mu_tilde_norms > 1e-12) & (mu_perp_norms > 1e-12)

        if torch.any(valid_indices):
            # Normalize mu_perp for valid indices
            mu_perp_normalized = torch.zeros_like(mu_perp)
            mu_perp_normalized[:, valid_indices] = mu_perp[:, valid_indices] / mu_perp_norms[valid_indices].unsqueeze(0)

            # Apply the update formula
            delta = mu_tilde_norms[valid_indices] * tan_theta
            U_prime = mu_tilde.clone()
            U_prime[:, valid_indices] += delta.unsqueeze(0) * mu_perp_normalized[:, valid_indices]

            # Normalize the updated vectors
            U_prime_norms = torch.norm(U_prime, dim=0)
            U_new[:, valid_indices] = U_prime[:, valid_indices] / U_prime_norms[valid_indices].unsqueeze(0)

        return U_new

    def find_points_outside_cone(self, X, U):
        """
        Find indices of points in X that are outside the cone generated by U.
        PyTorch implementation using batched operations.
        """
        m, n = X.size()
        r = U.size(1)

        # For large datasets, first try with a sample
        if n > 1000 and self.batch_size < n:
            # Sample a subset of points to check
            sample_indices = torch.randperm(n)[:min(self.batch_size, n)]
            X_sample = X[:, sample_indices]

            # Check the sample using batches
            outside_sample_mask = self._batch_check_outside_cone(X_sample, U)

            # Convert torch mask to indices
            outside_sample = torch.nonzero(outside_sample_mask).squeeze(-1)

            if len(outside_sample) == 0:
                # If no points in sample are outside, check a few more random points
                extra_indices = torch.randperm(n)[:min(self.batch_size, n)]
                X_extra = X[:, extra_indices]
                outside_extra_mask = self._batch_check_outside_cone(X_extra, U)
                outside_extra = torch.nonzero(outside_extra_mask).squeeze(-1)

                if len(outside_extra) == 0:
                    # If still no points outside, all points are likely inside
                    return []
                else:
                    # Convert sample indices back to original indices
                    return extra_indices[outside_extra].cpu().numpy().tolist()
            else:
                # Convert sample indices back to original indices
                return sample_indices[outside_sample].cpu().numpy().tolist()

        # For smaller datasets, check all points in batches
        outside_mask = self._batch_check_outside_cone(X, U)
        outside_indices = torch.nonzero(outside_mask).squeeze(-1).cpu().numpy().tolist()

        return outside_indices

    def _batch_check_outside_cone(self, X, U):
        """
        Check if points in X are outside the cone generated by U using batch operations.
        Returns a boolean mask where True indicates points outside the cone.
        """
        m, n = X.size()
        r = U.size(1)

        # Process in batches to avoid memory issues
        outside_mask = torch.zeros(n, dtype=torch.bool, device=self.device)

        for start_idx in range(0, n, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n)
            X_batch = X[:, start_idx:end_idx]

            # Compute UᵀU once
            UTU = torch.matmul(U.t(), U)

            # Add small regularization for numerical stability
            reg = 1e-8 * torch.eye(r, device=self.device)
            UTU_reg = UTU + reg

            # Compute UᵀX for the batch
            UTX_batch = torch.matmul(U.t(), X_batch)

            # Solve the system of linear equations
            try:
                # Try solving the system directly
                V_batch = torch.linalg.solve(UTU_reg, UTX_batch)
            except:
                # Fall back to using the pseudo-inverse
                UTU_pinv = torch.linalg.pinv(UTU)
                V_batch = torch.matmul(UTU_pinv, UTX_batch)

            # Check for negative coefficients
            has_negative = torch.any(V_batch < -self.tol, dim=0)
            outside_mask[start_idx:end_idx] = has_negative

        return outside_mask

    def compute_coefficients(self, X, U):
        """
        Compute the coefficient matrix V such that X ≈ UV using PyTorch.
        Uses non-negative least squares optimization.
        """
        m, n = X.size()
        r = U.size(1)

        # Initialize V
        V = torch.zeros((r, n), device=self.device)

        # Process in batches
        for start_idx in range(0, n, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n)
            X_batch = X[:, start_idx:end_idx]

            # Try the direct approach for solving non-negative least squares
            V_batch = self._solve_nnls_batch(U, X_batch)
            V[:, start_idx:end_idx] = V_batch

        return V

    def _solve_nnls_batch(self, U, X_batch):
        """
        Solve a batch of non-negative least squares problems using PyTorch.
        Implements a projected gradient descent algorithm.
        """
        m, n_batch = X_batch.size()
        r = U.size(1)

        # Initialize with small positive values
        V_batch = torch.ones((r, n_batch), device=self.device) * 0.01

        # Precompute U^T U and U^T X for efficiency
        UTU = torch.matmul(U.t(), U)
        UTX = torch.matmul(U.t(), X_batch)

        # Use gradient descent with projection
        learning_rate = 0.1
        max_iter = 200

        for i in range(max_iter):
            # Gradient = 2(U^T U V - U^T X)
            grad = 2 * (torch.matmul(UTU, V_batch) - UTX)

            # Update V
            V_batch = V_batch - learning_rate * grad

            # Project onto the non-negative orthant
            V_batch = torch.clamp(V_batch, min=0.0)

            # Check convergence
            if i > 0 and i % 10 == 0:
                error = torch.norm(grad)
                if error < self.tol:
                    break

                # Adjust learning rate if needed
                if i % 50 == 0:
                    learning_rate *= 0.5

        return V_batch

    def eliminate_redundant_rays(self, U, X, r):
        """
        Eliminate redundant vertex rays to keep only the most important r rays.
        PyTorch implementation with efficiency optimizations.
        """
        if U.size(1) <= r:
            return U

        # Compute importance scores for each ray
        k = U.size(1)
        importance = torch.zeros(k, device=self.device)

        # Calculate the full projection error as baseline
        X_proj = torch.matmul(U, torch.matmul(U.t(), X))
        baseline_error = torch.sum((X - X_proj) ** 2)

        # Compute importance in batches to avoid memory issues
        batch_size = min(5, k)  # Process rays in small batches

        for start_idx in range(0, k, batch_size):
            end_idx = min(start_idx + batch_size, k)

            for j in range(start_idx, end_idx):
                # Create a mask to select all columns except the jth one
                mask = torch.ones(k, dtype=torch.bool, device=self.device)
                mask[j] = False

                # Get the reduced U without the jth ray
                U_reduced = U[:, mask]

                # Calculate the projection error without this ray
                X_proj_reduced = torch.matmul(U_reduced, torch.matmul(U_reduced.t(), X))
                error_without_j = torch.sum((X - X_proj_reduced) ** 2)

                # The importance is how much the error increases if we remove this ray
                importance[j] = error_without_j - baseline_error

        # Keep the r most important rays
        _, indices = torch.topk(importance, r)
        U_reduced = U[:, indices]

        return U_reduced

    def fit(self, X_np, r=None):
        """
        Fit the Cone Collapsing algorithm to data X using PyTorch.

        Args:
            X_np (numpy.ndarray): Data matrix of shape (m, n) with nonnegative entries.
            r (int, optional): Target rank for factorization. If None, determined automatically.

        Returns:
            U (numpy.ndarray): Basis matrix of shape (m, r).
            V (numpy.ndarray): Coefficient matrix of shape (r, n).
        """
        start_time = time.time()

        # Convert numpy arrays to PyTorch tensors
        X = torch.tensor(X_np, dtype=torch.float32, device=self.device)
        m, n = X.size()

        # Ensure X is nonnegative
        if torch.any(X < 0):
            raise ValueError("Input matrix X must be nonnegative")

        # Calculate the center of the data (data mean)
        mu = torch.mean(X, dim=1)

        # Initialize with the standard basis (the nonnegative orthant)
        U = torch.eye(m, device=self.device)

        # Keep track of iterations and convergence
        iter_num = 0
        added_points = set()  # Use set for faster lookups
        prev_error = float('inf')
        no_improvement_count = 0

        if self.plot and m == 3:
            # Create plots directory if it doesn't exist
            os.makedirs('plots', exist_ok=True)
            # Plot initial state (convert to numpy for plotting)
            plotting(X_np, U.cpu().numpy(), iter_num)

        while iter_num < self.max_iters:
            iter_num += 1

            # Update theta if using adaptive approach
            if self.adaptive_theta:
                self.theta = max(0.01, self.theta * 0.98)

            if self.verbose and iter_num % 5 == 0:
                print(f"Iteration {iter_num}, U shape: {U.size()}, Theta: {self.theta:.4f}")

            # Step 1: Shrink the cone by tilting all vertex rays towards mu
            U = self.batch_update_vectors(U, mu, self.theta)

            # Step 2: Find points that fall outside the cone
            outside_points = self.find_points_outside_cone(X, U)

            if len(outside_points) == 0:
                if self.verbose and iter_num % 5 == 0:
                    print("All points inside the cone, continuing to shrink...")

                # Early stopping check
                if self.early_stopping:
                    # Compute coefficients and check error
                    V = self.compute_coefficients(X, U)
                    X_recon = torch.matmul(U, V)
                    current_error = torch.norm(X - X_recon) / torch.norm(X)
                    current_error = current_error.item()  # Convert to Python scalar

                    # Check for improvement
                    if prev_error - current_error < self.tol:
                        no_improvement_count += 1
                        if no_improvement_count >= 3:  # Stop after 3 iterations with little improvement
                            if self.verbose:
                                print(f"Early stopping at iteration {iter_num}, error: {current_error:.6f}")
                            break
                    else:
                        no_improvement_count = 0

                    prev_error = current_error

                if self.plot and m == 3 and iter_num % 5 == 0:
                    plotting(X_np, U.cpu().numpy(), iter_num)
                continue

            if self.verbose:
                print(f"Found {len(outside_points)} points outside the cone.")

            # Step 3: Add the points that are outside the cone as new vertex rays
            new_points_added = 0
            for point_idx in outside_points:
                if point_idx in added_points:
                    continue  # Skip if we've already added this point

                # Normalize the point and add it as a new vertex ray
                new_ray = X[:, point_idx] / torch.norm(X[:, point_idx])
                U = torch.cat((U, new_ray.unsqueeze(1)), dim=1)
                added_points.add(point_idx)
                new_points_added += 1

                # If we've added many points, check if we should eliminate some
                if r is not None and U.size(1) > r + 5:  # Add some buffer
                    break

            if new_points_added > 0 and self.verbose:
                print(f"Added {new_points_added} new vertex rays")

            # Optional: Periodically eliminate redundant vertex rays
            if r is not None and U.size(1) > r:
                U = self.eliminate_redundant_rays(U, X, r)

            if self.plot and m == 3 and (new_points_added > 0 or iter_num % 5 == 0):
                plotting(X_np, U.cpu().numpy(), iter_num)

            # Check if we've reached our target rank
            if r is not None and U.size(1) >= r and new_points_added == 0:
                if self.verbose:
                    print(f"Reached target rank {r} with no new points, stopping iterations.")
                break

        # Final elimination to ensure we have exactly r rays if specified
        if r is not None and U.size(1) > r:
            U = self.eliminate_redundant_rays(U, X, r)

        # Compute the coefficient matrix V
        V = self.compute_coefficients(X, U)

        # Calculate final error
        X_recon = torch.matmul(U, V)
        final_error = (torch.norm(X - X_recon) / torch.norm(X)).item()

        if self.verbose:
            end_time = time.time()
            print(f"Fitting completed in {end_time - start_time:.2f} seconds")
            print(f"Final U shape: {U.size()}")
            print(f"Final reconstruction error: {final_error:.6f}")

        # Convert back to numpy
        U_np = U.cpu().numpy()
        V_np = V.cpu().numpy()

        return U_np, V_np

    def reconstruct(self, U, V):
        """
        Reconstruct the data matrix from U and V.
        """
        if isinstance(U, np.ndarray):
            U = torch.tensor(U, dtype=torch.float32, device=self.device)
        if isinstance(V, np.ndarray):
            V = torch.tensor(V, dtype=torch.float32, device=self.device)

        X_recon = torch.matmul(U, V)
        return X_recon.cpu().numpy()