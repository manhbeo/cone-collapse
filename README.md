# Cone Collapse: Nonnegative Matrix Factorization through Cone Geometry

This repository contains the reference implementation of **Cone Collapse** and **CC–NMF**, the methods introduced in:

> *Nonnegative Matrix Factorization through Cone Collapse*  
> Manh Nguyen, Daniel Pimentel-Alarcón :contentReference[oaicite:0]{index=0}

Cone Collapse is a geometric algorithm that explicitly recovers the **minimal convex cone** generated by nonnegative data, and then uses this cone as the basis for an **orthogonal nonnegative matrix factorization (ONMF)** model for clustering.

---

## Overview

Given a nonnegative data matrix

$$
X \in \mathbb{R}_+^{m \times n},
$$

standard NMF seeks a low-rank approximation

$$
X \approx WH, \quad W \in \mathbb{R}_+^{m \times r}, \; H \in \mathbb{R}_+^{r \times n}.
$$

Geometrically, the columns of $X$ lie in the **convex cone** generated by the columns of $W$:

$$
\mathrm{cone}(W) = \{ \sum_{k=1}^r \alpha_k w_k : \alpha_k \ge 0 \}.
$$

Cone Collapse revisits NMF from this conic viewpoint:

1. **Recover the data cone**  
   Treat the rows of $X$ as points in $\mathbb{R}^n_+$ (i.e., work with $X^\top$) and iteratively shrink the full nonnegative orthant to the **smallest cone that still contains all data points**.

2. **Build a cone-aware ONMF model (CC–NMF)**  
   Use the recovered extreme rays as a geometric basis, then apply a uni-orthogonal NMF to obtain an orthogonal factorization whose latent factors are directly tied to the extreme rays of the data cone. :contentReference[oaicite:1]{index=1}

The paper shows that, under mild assumptions, Cone Collapse **terminates in finitely many steps** and recovers the minimal generating cone of $X^\top$, and that CC–NMF yields strong clustering performance on gene-expression, text, and image benchmarks.

---

## Method

### 1. Cone Collapse (Extreme-Ray Recovery)

Let $X^\top = [x_1, \dots, x_m] \in \mathbb{R}^{n \times m}_+$ be the data points (rows of $X$). Cone Collapse aims to find a matrix

$$
U^\star = [u_1, \dots, u_c] \in \mathbb{R}^{n \times c}_+
$$

whose columns are (a subset of) the **extreme rays** of $\mathrm{cone}(X^\top)$.

High-level algorithm (Algorithm 1 in the paper):

1. **Initialize**

   - Compute the mean direction of the data:
     $$
     \mu = \frac{1}{m} \sum_{i=1}^m x_i.
     $$
   - Start from the full nonnegative orthant:
     $$
     U^{(0)} = I_n.
     $$

2. **Mean tilt step**

   For each column $u^{(t)}_k$ of $U^{(t)}$:

   - If $u^{(t)}_k$ is colinear with some data point $x_i$, keep it.  
   - Otherwise, “tilt” it towards the normalized mean $\bar{\mu}$:
     $$
     u^{(t)}_k \leftarrow (1 - \eta)\, u^{(t)}_k + \eta\, \bar{\mu},
     $$
     then renormalize to unit norm.

3. **Add outside points**

   Solve a nonnegative least squares (NNLS) problem

   $$
   \min_{H \ge 0} \|X^\top - \tilde{U}^{(t)} H\|_F^2
   $$

   to represent the data in the current cone. Any data point whose residual is larger than a tolerance (i.e., lies significantly **outside** the cone) is added as a new ray (normalized row of $X$).

4. **Remove redundant rays**

   For each column $\tilde{u}_k$ of $\tilde{U}^{(t)}$, solve a small NNLS

   $$
   \min_{w \ge 0} \|\tilde{u}_k - \tilde{U}^{(t)}_{-k} w\|_2^2.
   $$

   If the residual is below a tolerance, $\tilde{u}_k$ lies in the cone of the remaining columns and is **redundant**; remove it.

5. **Repeat**

   Set $U^{(t+1)} \leftarrow \tilde{U}^{(t)}$ and repeat the steps until every extreme ray is aligned with some data point and no further changes occur.

The paper proves that:

- Cone Collapse **adds all missing extreme rays** in finitely many iterations.
- All non-extreme rays are eventually pruned.  
- The final $U^{(T)}$ satisfies
  $$
  \mathrm{cone}(U^{(T)}) = \mathrm{cone}(X^\top).
  $$ :contentReference[oaicite:2]{index=2}

Internally, all NNLS problems are solved by a **block principal pivoting (BPP)** method for single and multiple right-hand sides (Algorithms 2 and 3 in the appendix).

---

### 2. CC–NMF: Cone-Aware Orthogonal NMF

Once the extreme-ray basis $U^\star$ is recovered, CC–NMF builds an ONMF model in two steps. :contentReference[oaicite:3]{index=3}

1. **Recover coefficients for $X$**

   Solve

   $$
   V^\star = \arg\min_{V \ge 0} \|X^\top - U^\star V\|_F^2.
   $$

   Then $X^\top \approx U^\star V^\star$ and hence $X \approx (V^\star)^\top (U^\star)^\top$.

2. **Uni-orthogonal factorization of $U^\star$**

   Solve a uni-orthogonal NMF problem

   $$
   \min_{A \ge 0,\, S \ge 0} \|U^\star - A S\|_F^2
   \quad \text{s.t.} \quad A^\top A = I_r,
   $$

   where:

   - $A \in \mathbb{R}^{n \times r}_+$ has orthonormal columns,
   - $S \in \mathbb{R}^{r \times c}_+$ has nonnegative loadings,
   - $r$ is the desired number of clusters (often set to the number of classes).

   The paper uses multiplicative updates (adapted from uni-orthogonal NMF):

   $$
   S \leftarrow S \odot \frac{A^\top U^\star}{(A^\top A) S}, \qquad
   A \leftarrow A \odot \frac{U^\star S^\top}{A A^\top U^\star S^\top},
   $$

   with periodic re-normalization of the columns of $A$ to keep $A^\top A \approx I_r$.

   Combining both stages, we obtain

   $$
   X \approx (V^\star)^\top S^\top A^\top,
   $$

   so a valid ONMF of $X$ is

   $$
   W := (V^\star)^\top S^\top \in \mathbb{R}_+^{m \times r}, \quad
   H := A^\top \in \mathbb{R}_+^{r \times n},
   $$

   with $H H^\top = I_r$.

For clustering, each sample (column of $X$) is assigned to the cluster with the largest entry in the corresponding column of $H$.
